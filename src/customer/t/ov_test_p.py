#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import time
import math
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import List, Tuple, Optional

import numpy as np
from PIL import Image
from transformers import AutoProcessor
from openvino.runtime import Core, AsyncInferQueue

# =========================
# åŸºæœ¬é…ç½®ï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰
# =========================
hf_id = "/home/xtang/vdb-sandbox/models/embedding/models/openai/clip-vit-base-patch32"
ov_dir = Path("./ov_models/clip-vit-base-patch32")
device_ov = "CPU"

# Flickr8k è·¯å¾„
flickr_img_dir = "./datasets/Flickr8k/Flicker8k_Dataset/"
flickr_token_txt = "./datasets/Flickr8k/Flickr8k.token.txt"

# éšæœº DEMO å‚æ•°
rand_batch_size = 200
rand_img_hw = (224, 224)  # (H, W)
rand_seed = 42


# =========================
# å·¥å…·å‡½æ•°ï¼ˆNumPyï¼‰
# =========================
def mean_pooling_np(last_hidden_state: np.ndarray, attention_mask: np.ndarray) -> np.ndarray:
    mask = attention_mask[..., None].astype(last_hidden_state.dtype)
    summed = (last_hidden_state * mask).sum(axis=1)
    denom = np.clip(mask.sum(axis=1), 1e-9, None)
    return summed / denom

def l2_normalize_np(x: np.ndarray, axis: int = -1, eps: float = 1e-12) -> np.ndarray:
    return x / np.clip(np.linalg.norm(x, ord=2, axis=axis, keepdims=True), eps, None)


# =========================
# OpenVINO Runtimeï¼šè½½å…¥å·²å¯¼å‡ºçš„ IR
# =========================
def load_ov_compiled(ov_dir: Path, device: str = "CPU"):
    xml = ov_dir / "openvino_model.xml"
    bin_ = ov_dir / "openvino_model.bin"
    assert xml.exists() and bin_.exists(), f"åœ¨ {ov_dir} ä¸‹æ‰¾ä¸åˆ° openvino_model.xml/.bin"
    core = Core()
    model = core.read_model(xml.as_posix(), bin_.as_posix())
    compiled = core.compile_model(model, device)
    return compiled


# =========================
# Flickr8k æ•°æ®åŠ è½½
# =========================
def load_flickr8k_pairs(image_dir: str, token_txt: str, limit: Optional[int] = None) -> Tuple[List[Path], List[str]]:
    image_dir = Path(image_dir)
    token_file = Path(token_txt)
    assert image_dir.exists() and token_file.exists(), "è¯·æ£€æŸ¥ Flickr8k æ•°æ®è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚"

    img2caps = {}
    with token_file.open("r", encoding="utf-8") as f:
        for line in f:
            img_tag, caption = line.strip().split("\t")
            img = img_tag.split("#")[0]
            img2caps.setdefault(img, []).append(caption)

    image_paths, texts = [], []
    for img, caps in sorted(img2caps.items()):
        p = image_dir / img
        if p.exists():
            image_paths.append(p)
            texts.append(caps[0])  # æ¯å¼ å›¾å–ç¬¬ 1 æ¡
            if limit and len(image_paths) >= limit:
                break
    print(f"âœ… Flickr8k åŠ è½½æ ·æœ¬æ•°ï¼š{len(image_paths)}")
    return image_paths, texts


# =========================
# æ¨ç†è¾…åŠ©
# =========================
def _compiled_feed(compiled, ids: np.ndarray, mask: np.ndarray, pixel: np.ndarray):
    feed = {}
    for inp in compiled.inputs:
        n = inp.get_any_name().lower()
        if "pixel" in n or "image" in n:
            feed[inp] = pixel
        elif "mask" in n:
            feed[inp] = mask
        elif "input" in n and "id" in n:
            feed[inp] = ids
    # å…œåº•ï¼ˆæå°‘æ•°æ¨¡å‹åå­—ä¸å«å…³é”®è¯ï¼‰
    if len(feed) < 3:
        for inp in compiled.inputs:
            if inp not in feed:
                shp = tuple(inp.get_shape())
                if len(shp) == 4:
                    feed[inp] = pixel
                elif len(shp) == 2 and shp[1] == ids.shape[1]:
                    if "mask" in inp.get_any_name().lower():
                        feed[inp] = mask
                    else:
                        feed[inp] = ids
    return feed

def _extract_embeds(compiled, res_dict, mask: np.ndarray):
    img_emb = txt_emb = None
    lh_vis = lh_text = lh_common = None

    for out in compiled.outputs:
        n = out.get_any_name().lower()
        v = np.array(res_dict[out])
        if "image_embeds" in n:
            img_emb = v
        elif "text_embeds" in n:
            txt_emb = v
        elif "vision" in n and "last_hidden_state" in n:
            lh_vis = v
        elif "text" in n and "last_hidden_state" in n:
            lh_text = v
        elif "last_hidden_state" in n:
            lh_common = v

    if img_emb is None:
        if lh_vis is not None:
            img_emb = lh_vis.mean(axis=1)
        elif lh_common is not None:
            img_emb = lh_common.mean(axis=1)
        else:
            raise RuntimeError("æ— æ³•ä»è¾“å‡ºä¸­è·å–å›¾åƒç‰¹å¾ã€‚")

    if txt_emb is None:
        if lh_text is not None:
            txt_emb = mean_pooling_np(lh_text, mask)
        elif lh_common is not None:
            txt_emb = mean_pooling_np(lh_common, mask)
        else:
            raise RuntimeError("æ— æ³•ä»è¾“å‡ºä¸­è·å–æ–‡æœ¬ç‰¹å¾ã€‚")

    return l2_normalize_np(img_emb), l2_normalize_np(txt_emb)


# =========================
# å•çº¿ç¨‹ç¼–ç 
# =========================
def encode_with_ov(compiled, processor, image_paths: List[Path], texts: List[str], batch_size=32):
    t0 = time.time()
    N = len(image_paths)
    img_all, txt_all = [], []
    for i in range(0, N, batch_size):
        j = min(i + batch_size, N)
        imgs = [Image.open(p).convert("RGB") for p in image_paths[i:j]]
        caps = texts[i:j]
        both = processor(text=caps, images=imgs, return_tensors="np", padding=True)
        pixel = both["pixel_values"].astype("float32")
        ids   = both["input_ids"].astype("int64")
        mask  = both["attention_mask"].astype("int64")
        if pixel.ndim == 4 and pixel.shape[1] not in (1, 3) and pixel.shape[-1] in (1, 3):
            pixel = np.transpose(pixel, (0, 3, 1, 2)).copy()
        feed = _compiled_feed(compiled, ids, mask, pixel)
        res = compiled(feed)
        img, txt = _extract_embeds(compiled, res, mask)
        img_all.append(img)
        txt_all.append(txt)
    img_embs = np.concatenate(img_all)
    txt_embs = np.concatenate(txt_all)
    t1 = time.time()
    print(f"âš™ï¸ ç¼–ç å®Œæˆ: {N} å¯¹æ ·æœ¬, è€—æ—¶ {t1 - t0:.3f}s, å¹³å‡ {(t1 - t0)/N*1000:.2f} ms/æ ·æœ¬")
    return img_embs, txt_embs


# =========================
# å¤šçº¿ç¨‹ç¼–ç ï¼ˆçº¿ç¨‹æ±  + InferRequest/çº¿ç¨‹ï¼‰
# =========================
def _encode_shard_with_infer_request(
    compiled,
    processor,
    image_paths: List[Path],
    texts: List[str],
    start: int,
    end: int,
    batch_size: int = 32,
):
    infer_request = compiled.create_infer_request()
    img_chunks, txt_chunks = [], []
    for i in range(start, end, batch_size):
        j = min(i + batch_size, end)
        imgs = [Image.open(p).convert("RGB") for p in image_paths[i:j]]
        caps = texts[i:j]
        both = processor(text=caps, images=imgs, return_tensors="np", padding=True)
        pixel = both["pixel_values"].astype("float32", copy=False)
        ids   = both["input_ids"].astype("int64",   copy=False)
        mask  = both["attention_mask"].astype("int64", copy=False)
        if pixel.ndim == 4 and pixel.shape[1] not in (1, 3) and pixel.shape[-1] in (1, 3):
            pixel = np.transpose(pixel, (0, 3, 1, 2)).copy()
        feed = _compiled_feed(compiled, ids, mask, pixel)
        res  = infer_request.infer(feed)
        img_emb, txt_emb = _extract_embeds(compiled, res, mask)
        img_chunks.append(img_emb)
        txt_chunks.append(txt_emb)
    return np.concatenate(img_chunks, axis=0), np.concatenate(txt_chunks, axis=0)

def encode_with_ov_parallel(
    compiled,
    processor,
    image_paths: List[Path],
    texts: List[str],
    batch_size: int = 32,
    workers: int = 2,
):
    N = len(image_paths)
    workers = max(1, int(workers))
    if workers == 1 or N <= batch_size:
        return encode_with_ov(compiled, processor, image_paths, texts, batch_size=batch_size)

    # åˆ‡åˆ†æ ·æœ¬
    shard_ranges = []
    per = math.ceil(N / workers)
    for k in range(workers):
        s = k * per
        e = min(N, (k + 1) * per)
        if s < e:
            shard_ranges.append((s, e))

    img_parts, txt_parts = [None] * len(shard_ranges), [None] * len(shard_ranges)

    t0 = time.time()
    with ThreadPoolExecutor(max_workers=workers) as ex:
        futures = {}
        for idx, (s, e) in enumerate(shard_ranges):
            fut = ex.submit(
                _encode_shard_with_infer_request,
                compiled, processor, image_paths, texts, s, e, batch_size
            )
            futures[fut] = idx
        for fut in as_completed(futures):
            idx = futures[fut]
            img_shard, txt_shard = fut.result()
            img_parts[idx] = img_shard
            txt_parts[idx] = txt_shard

    img_embs = np.concatenate(img_parts, axis=0)
    txt_embs = np.concatenate(txt_parts, axis=0)
    t1 = time.time()
    print(f"âš™ï¸ å¹¶è¡Œç¼–ç å®Œæˆ: {N} å¯¹, çº¿ç¨‹ {workers} ä¸ª, è€—æ—¶ {t1 - t0:.3f}s, å¹³å‡ {(t1 - t0)/N*1000:.2f} ms/æ ·æœ¬")
    return img_embs, txt_embs


# =========================
# AsyncInferQueue å¼‚æ­¥ç¼–ç 
# =========================
def encode_with_ov_async(
    compiled,
    processor,
    image_paths: List[Path],
    texts: List[str],
    batch_size: int = 32,
    workers: int = 2,
):
    """
    ç”¨ OpenVINO AsyncInferQueue åšå¤šè¯·æ±‚å¼‚æ­¥æ¨ç†
    """
    N = len(image_paths)
    if N == 0:
        return np.zeros((0, 1)), np.zeros((0, 1))
    workers = max(1, int(workers))

    # 1) é¢„å¤„ç† -> feed åˆ—è¡¨
    batches = []
    for i in range(0, N, batch_size):
        j = min(i + batch_size, N)
        imgs = [Image.open(p).convert("RGB") for p in image_paths[i:j]]
        caps = texts[i:j]
        both = processor(text=caps, images=imgs, return_tensors="np", padding=True)
        pixel = both["pixel_values"].astype("float32", copy=False)
        ids   = both["input_ids"].astype("int64",   copy=False)
        mask  = both["attention_mask"].astype("int64", copy=False)
        if pixel.ndim == 4 and pixel.shape[1] not in (1, 3) and pixel.shape[-1] in (1, 3):
            pixel = np.transpose(pixel, (0, 3, 1, 2)).copy()
        feed = _compiled_feed(compiled, ids, mask, pixel)
        batches.append((feed, mask))

    num_batches = len(batches)
    img_parts = [None] * num_batches
    txt_parts = [None] * num_batches

    # 2) AIQ + å›è°ƒ
    aiq = AsyncInferQueue(compiled, jobs=workers)

    def _cb(request, userdata):
        bidx, mask_np = userdata  # å“ªä¸ª batch
        res = request.results
        img_emb, txt_emb = _extract_embeds(compiled, res, mask_np)
        img_parts[bidx] = img_emb
        txt_parts[bidx] = txt_emb

    aiq.set_callback(_cb)

    # 3) å¯åŠ¨ + ç­‰å¾…
    t0 = time.time()
    for bidx, (feed, mask) in enumerate(batches):
        aiq.start_async(feed, userdata=(bidx, mask))
    aiq.wait_all()
    t1 = time.time()

    img_embs = l2_normalize_np(np.concatenate(img_parts, axis=0), axis=-1)
    txt_embs = l2_normalize_np(np.concatenate(txt_parts, axis=0), axis=-1)
    print(f"âš™ï¸ AsyncInferQueue ç¼–ç å®Œæˆ: {N} å¯¹, é˜Ÿåˆ— {workers}, è€—æ—¶ {t1 - t0:.3f}s, å¹³å‡ {(t1 - t0)/N*1000:.2f} ms/æ ·æœ¬")
    return img_embs, txt_embs


# =========================
# æ£€ç´¢æ‰“å°
# =========================
def retrieval_print(img_embs: np.ndarray, txt_embs: np.ndarray, image_paths: List[Path], texts: List[str], topk=3):
    sims_t2i = txt_embs @ img_embs.T
    sims_i2t = img_embs @ txt_embs.T

    print("\n===== å›¾åƒ â†’ æ–‡æœ¬ (Image â†’ Text) =====")
    for i in range(min(3, len(image_paths))):
        idx = np.argsort(-sims_i2t[i])[:topk]
        print(f"[Image {i}] {image_paths[i].name}")
        for r, j in enumerate(idx, 1):
            print(f"  {r}. '{texts[j]}'  sim={sims_i2t[i, j]:.4f}")

    print("\n===== æ–‡æœ¬ â†’ å›¾åƒ (Text â†’ Image) =====")
    for i in range(min(3, len(texts))):
        idx = np.argsort(-sims_t2i[i])[:topk]
        print(f"[Text {i}] '{texts[i]}'")
        for r, j in enumerate(idx, 1):
            print(f"  {r}. {image_paths[j].name}  sim={sims_t2i[i, j]:.4f}")


# =========================
# éšæœº DEMOï¼ˆå•çº¿ç¨‹ / AIQï¼‰
# =========================
def random_demo(processor, compiled, batch_size=8, img_hw=(224, 224), seed=42):
    t_start = time.time()
    np.random.seed(seed)
    np_imgs = [np.random.randint(0, 255, (img_hw[0], img_hw[1], 3), dtype=np.uint8) for _ in range(batch_size)]
    pil_imgs = [Image.fromarray(a) for a in np_imgs]
    texts = [f"random text #{i}" for i in range(batch_size)]

    both = processor(text=texts, images=pil_imgs, return_tensors="np", padding=True)
    pixel = both["pixel_values"].astype("float32")
    ids   = both["input_ids"].astype("int64")
    mask  = both["attention_mask"].astype("int64")
    if pixel.ndim == 4 and pixel.shape[1] not in (1, 3) and pixel.shape[-1] in (1, 3):
        pixel = np.transpose(pixel, (0, 3, 1, 2)).copy()
    feed  = _compiled_feed(compiled, ids, mask, pixel)

    t_infer0 = time.time()
    res = compiled(feed)
    t_infer1 = time.time()

    img, txt = _extract_embeds(compiled, res, mask)
    sims = (img @ txt.T)

    t_total = time.time() - t_start
    print(f"âš™ï¸ DEMO æ¨ç†è€—æ—¶ {t_infer1 - t_infer0:.3f}sï¼Œæ€»è€—æ—¶ {t_total:.3f}s")
    for i in range(min(3, batch_size)):
        idx = np.argsort(-sims[i])[:3]
        print(f"[Rand Image #{i}] top-3 text matches:", ", ".join([f"{j}:{sims[i,j]:.3f}" for j in idx]))
    return t_total

def random_demo_async(processor, compiled, batch_size=8, img_hw=(224,224), seed=42, workers=2):
    np.random.seed(seed)
    np_imgs = [np.random.randint(0, 255, (img_hw[0], img_hw[1], 3), dtype=np.uint8) for _ in range(batch_size)]
    pil_imgs = [Image.fromarray(a) for a in np_imgs]
    texts = [f"random text #{i}" for i in range(batch_size)]

    both = processor(text=texts, images=pil_imgs, return_tensors="np", padding=True)
    pixel = both["pixel_values"].astype("float32")
    ids   = both["input_ids"].astype("int64")
    mask  = both["attention_mask"].astype("int64")
    feed  = _compiled_feed(compiled, ids, mask, pixel)

    aiq = AsyncInferQueue(compiled, jobs=workers)
    holder = [None] * workers

    def _cb(req, userdata):
        holder[userdata] = req.results

    aiq.set_callback(_cb)

    t0 = time.time()
    # æŠŠ batch åˆ‡åˆ†ä¸º workers æ®µ
    parts = np.array_split(np.arange(batch_size), workers)
    for idx, ids in enumerate(parts):
        sub_feed = {k: (v[ids] if hasattr(v, "__getitem__") and len(v) == batch_size else v)
                    for k, v in feed.items()}
        aiq.start_async(sub_feed, userdata=idx)

    aiq.wait_all()
    t1 = time.time()

    # èšåˆç»“æœ
    all_img, all_txt = [], []
    for res in holder:
        if res is not None:
            img, txt = _extract_embeds(compiled, res, mask)
            all_img.append(img)
            all_txt.append(txt)
    img = l2_normalize_np(np.concatenate(all_img), axis=-1)
    txt = l2_normalize_np(np.concatenate(all_txt), axis=-1)
    sims = img @ txt.T

    print(f"âš™ï¸ DEMO (AsyncInferQueue True Parallel) è€—æ—¶ {t1 - t0:.3f}sï¼Œbatch={batch_size}, workers={workers}")
    for i in range(min(3, batch_size)):
        idx = np.argsort(-sims[i])[:3]
        print(f"[Rand Image #{i}] top-3 text matches:",
              ", ".join([f"{j}:{sims[i,j]:.3f}" for j in idx]))
    return t1 - t0



# =========================
# ä¸»ç¨‹åº
# =========================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="CLIP-OpenVINO å›¾æ–‡æ£€ç´¢ / è®¡æ—¶ / å¹¶è¡Œ (threads/async)")
    parser.add_argument("--mode", choices=["demo", "flickr"], default="demo",
                        help="demoï¼šéšæœºï¼›flickrï¼šç”¨ Flickr8k å®æµ‹")
    parser.add_argument("--limit", type=int, default=100, help="flickr æ¨¡å¼ä¸‹é™åˆ¶æ ·æœ¬æ•°")
    parser.add_argument("--loops", type=int, default=1, help="å¾ªç¯æ¬¡æ•°åšå¹³å‡")
    parser.add_argument("--workers", type=int, default=1, help="å¹¶è¡Œåº¦ï¼šthreads çš„çº¿ç¨‹æ•°æˆ– async çš„é˜Ÿåˆ—å¤§å°")
    parser.add_argument("--batch-size", type=int, default=32, help="ç¼–ç  batch sizeï¼ˆflickr æ¨¡å¼ï¼‰")
    parser.add_argument("--engine", choices=["threads", "async"], default="threads",
                        help="å¹¶è¡Œå¼•æ“ï¼šthreads=ThreadPoolExecutorï¼Œasync=OpenVINO AsyncInferQueue")
    args = parser.parse_args()

    processor = AutoProcessor.from_pretrained(hf_id)
    compiled = load_ov_compiled(ov_dir, device_ov)

    all_times = []

    for loop in range(args.loops):
        print(f"\n====== ç¬¬ {loop+1}/{args.loops} æ¬¡è¿è¡Œ ({args.mode}, engine={args.engine}, workers={args.workers}) ======")
        t0 = time.time()

        if args.mode == "demo":
            if args.engine == "async":
                total_time = random_demo_async(processor, compiled,
                                               batch_size=rand_batch_size,
                                               img_hw=rand_img_hw,
                                               seed=rand_seed,
                                               workers=args.workers)
            else:
                if args.workers > 1:
                    # çº¿ç¨‹æ± ç‰ˆ demoï¼šæŠŠ batch åˆ‡ç‰‡å¹¶è¡Œï¼ˆç®€å•æ¼”ç¤ºï¼‰
                    # ä¸ºä¿æŒè„šæœ¬ç®€æ´ï¼Œè¿™é‡Œç›´æ¥å¤ç”¨å•çº¿ç¨‹ demo çš„æ€»æ—¶é—´ä½œä¸ºå‚è€ƒ
                    total_time = random_demo(processor, compiled,
                                             batch_size=rand_batch_size,
                                             img_hw=rand_img_hw,
                                             seed=rand_seed)
                else:
                    total_time = random_demo(processor, compiled,
                                             batch_size=rand_batch_size,
                                             img_hw=rand_img_hw,
                                             seed=rand_seed)
        else:
            image_paths, texts = load_flickr8k_pairs(flickr_img_dir, flickr_token_txt, limit=args.limit)
            if args.engine == "async":
                img_embs, txt_embs = encode_with_ov_async(compiled, processor, image_paths, texts,
                                                          batch_size=args.batch_size, workers=args.workers)
            else:
                if args.workers > 1:
                    img_embs, txt_embs = encode_with_ov_parallel(compiled, processor, image_paths, texts,
                                                                 batch_size=args.batch_size, workers=args.workers)
                else:
                    img_embs, txt_embs = encode_with_ov(compiled, processor, image_paths, texts,
                                                        batch_size=args.batch_size)
            retrieval_print(img_embs, txt_embs, image_paths, texts)
            total_time = time.time() - t0
            print(f"ğŸ•’ æœ¬æ¬¡ Flickr8k æ€»è€—æ—¶: {total_time:.3f}s")

        all_times.append(total_time)

    # ç»Ÿè®¡
    print("\n====== æ€§èƒ½ç»Ÿè®¡ ======")
    for i, t in enumerate(all_times):
        print(f"  ç¬¬{i+1}æ¬¡: {t:.3f}s")
    avg_t = sum(all_times) / len(all_times)
    print(f"  å¹³å‡è€—æ—¶: {avg_t:.3f}s  ï¼ˆå¾ªç¯ {args.loops} æ¬¡, engine={args.engine}, workers={args.workers}ï¼‰")
